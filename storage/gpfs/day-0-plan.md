# GPFS 集群规划

## 概念

* failure group: GPFS 允许您将存储硬件组织成故障组。故障组定义为一组共享一个共同故障点的磁盘，该故障点可能导致它们同时变得不可用。

## 模式

三种集群模式：
* 存储区域网络 (SAN)
* 网络共享磁盘 (NSD)
* **shared nothing 无共享**

## 架构

* 管理功能: 一个节点可以包含多个管理功能

    * GPFS cluster manager: 每个管理器有一个 GPFS 集群，该管理器是从一组指定用于该集群的仲裁节点中选择的。

        * 监视磁盘租约
        * 检测故障并管理集群中节点故障的恢复
        * 分发远程集群中节点必须知道的某些配置更改
        * 选择文件系统管理器节点
        * 聚合来自集群中所有节点的运行状况信息
        * 执行 `mmlsmgr -c` 查看

    * File System manager: 每个文件系统有一个文件系统管理器

        * 文件系统配置: 添加磁盘; 更改磁盘可用性; 修复文件系统
        * 磁盘空间分配管理
        * 令牌管理: 文件系统管理器节点也可以执行令牌管理器服务器的职责。

            * 令牌管理服务器通过授予令牌来协调对共享磁盘上文件的访问，这些令牌传递读取或写入文件数据或元数据的权限
            * 每个令牌的状态保存在两个位置: 在令牌管理服务器上; 在持有令牌的令牌管理客户机上
            * 节点首次访问文件时，必须向令牌管理服务器发送请求，以获取相应的读取或写入令牌。授予令牌后，节点可以继续读取或写入文件，而无需与令牌管理服务器进行额外的交互。这种情况会一直持续到另一个节点上的应用程序尝试读取或写入文件中的同一区域。

        * 配额管理
        * `mmlsmgr` 和 `mmchmgr` 来管理

    * Metanode: 每个打开的文件都有一个元节点。在几乎所有情况下，持续打开文件时间最长的节点都是元节点。所有访问文件的节点都可以直接读取和写入数据，但元数据的更新只能由元节点写入。
    * CES node: 导出 NFS,SMB,S3和HDFS 等协议

        * 可以使用 `mmchnode --ces-enable Node` 命令将集群中的节点指定为 CES 节点。

* 在 GPFS 文件系统中使用磁盘存储和文件结构

    * 文件系统（或条带组）由一组磁盘组成，这些磁盘存储文件数据、文件元数据和支持实体，例如配额文件和恢复日志。
    * 使用 `mmcrfs` 命令创建文件系统，或者使用 `mmchdisk` 命令修改文件系统，从而控制 GPFS 使用哪些磁盘来存储元数据。
    * 如果文件足够小，其数据可以容纳在此空间内，那么数据可以存储在 inode 本身中。此方法称为 **data-in-inode**，可以提高使用许多小文件的工作负载的性能和空间利用率。
    * GPFS 集群中已挂载文件系统的最大数量为 256
    * 配额文件: 配额文件在通过 -Q yes 选项启用配额时创建，该选项可以在 mmcrfs 命令或 mmchfs 命令中设置。查询 `mmlsfs FSNAME -Q`

* GPFS 网络通信

    * 在集群环境中，GPFS 守护程序依赖于 TCP/IP 的正确运行

* 集群导出服务 CES

    * 至少 2 节点以满足 HA 需要
    * 执行 `mmces service enable/disable nfs` 来开启或禁用指定协议
    * nfs: `mmnfs config/export`
    * s3: `mms3 config/account/bucket` 和 `mmhealth node show`

* [产品](https://www.ibm.com/docs/en/storage-scale/5.2.2?topic=overview-storage-scale-product-editions)

    * IBM Storage Scale Data Access Edition: 基础版
    * IBM Storage Scale Data Management Edition: 相比基础版：异步多站点灾难恢复，文件审计日志记录
    * IBM Storage Scale Erasure Code Edition：相比数据管理版：纠删码

## 规划

* 可恢复性考虑

    * 节点故障

        * Quorum 仲裁

            GPFS 使用一种称为仲裁的集群机制，以便在节点发生故障时保持数据一致性。

            如果由于节点故障而无法保持仲裁，那么 GPFS 会卸载剩余节点上的本地文件系统，并尝试重新建立仲裁，此时会发生文件系统恢复。

            GPFS 可以使用以下两种方法之一来确定仲裁:
            * 节点仲裁: 节点法定数是 GPFS 的缺省法定数算法
            * 使用仲裁盘的节点仲裁: 在小型 GPFS 集群上运行时，您可能希望集群在只有一个幸存节点的情况下保持联机状态. **仲裁磁盘必须连接到所有仲裁节点**

        * 选择仲裁节点

            * 如果某个节点可能需要重新启动或进行维护，请不要选择该节点作为仲裁节点
            * 选择具有不同故障点的节点, 例如不同机柜节点
            * 选择奇数个节点作为仲裁节点：没有仲裁盘的集群最多可以配置九个仲裁节点。带有仲裁盘的集群最多只能有八个仲裁节点
            * 拥有大量仲裁节点可能会增加启动和故障恢复所需的时间：拥有超过七个仲裁节点并不能保证更高的可用性

    * NSD 服务器和磁盘故障

* GPFS 集群创建注意事项

    * 创建 IBM Storage Scale 集群

        * 运行 mmcrcluster 命令以创建一个包含一个或多个节点的集群，并在以后根据需要运行 mmaddnode 命令以将节点添加到集群
        * 使用多行格式描述节点 

            ```yaml
            NodeName:NodeDesignations:AdminNodeName
            ```
            * NodeName: 使用 short hostname, long hostname 或者 IP
            * NodeDesignations: 多个角色使用 `-` 分割, `manager | client` 指示节点是否属于可从中选择文件系统管理器和令牌管理器的节点池, `quorum | nonquorum` 指定项用于指定是否应将节点包括在从中派生仲裁的节点池中 
            * AdminNodeName: 指定一个可选字段，该字段包含一个节点名称，供管理命令在节点之间进行通信时使用

    * IBM Storage Scale 集群配置信息: Cluster Configuration Repository (CCR) 维护集群配置信息。CCR 使用基于 Paxos 的算法来保持存储数据在仲裁节点之间的一致性。
    * 远程 shell 命令: 缺省的远程 Shell 命令是 ssh
    * 远程文件复制命令: GPFS 命令必须在集群中的所有节点上维护多个配置文件。缺省的远程文件复制程序是 scp。
    * 集群名称: 通过在 mmcrcluster 命令上发出 -C 选项，为集群提供一个名称

* 磁盘注意事项

    * 网络共享磁盘 (NSD) 创建注意事项: 在使用 GPFS 之前，您必须先使用 mmcrnsd 命令将每个要使用的物理磁盘定义为网络共享磁盘 (NSD)

* 文件系统创建注意事项

    文件系统分配多少磁盘资源、选择哪个块大小、在何处存储数据和元数据，以及维护多少副本.

    您可以使用 mmcrfs 命令创建 GPFS 文件系统。

    * Block size 块大小：使用缺省 4MiB 块大小，可以满足大部分场景。（子块大小是 8KiB）
    * 文件系统复制参数 
